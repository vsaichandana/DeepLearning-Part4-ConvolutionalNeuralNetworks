{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2NP1Jm6grOA"
   },
   "source": [
    "# Homework 4: Building and Training Convolutional Networks\n",
    "When you download the file into your Google Drive, please insert your name and student ID number here by replacing my name in the fields below.\n",
    "\n",
    "Julie DICKERSON   ID: 123456789\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MD7n4BGOkbim"
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "Welcome to your fourth assignment! You have previously built and trained a deep Fully Connected (FC) Neural Network with L layers using Pytorch. In this  assignment, you will use PyTorch to put together a ConvNet and train it on the CIFAR-10 image data using a softmax backend.\n",
    "\n",
    "In this notebook, you will implement and train a CNN to learn what is in the ten classes of images. This will enable you to start learning the PyTorch framework for deep learning.\n",
    "\n",
    "*   Uses the PyTorch dataloader function\n",
    "*   Use PyTorch *nn* to create an instance of a convolutional network\n",
    "*   Visualization functions to see what is happening as the network goes deeper.\n",
    "\n",
    "**After this assignment you will be able to:**\n",
    "- Alter the structure of a model.\n",
    "- Explain how a ConvNet works with its successive layers.\n",
    "- Visualize results from a multiclass classifier using a heatmap and a confusion matrix.\n",
    "\n",
    "**There code that you need to fill in this assignment**\n",
    "See sections 3.1, 3.2, and 3.3.\n",
    "\n",
    "**There are questions that you need to answer in this assignment**\n",
    "The questions are in section 6!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13widXLVt4_Z"
   },
   "source": [
    "## 1.0 - Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eAckZ0skbir"
   },
   "source": [
    "\n",
    "Now, let's first import all the packages that you will need during this assignment.\n",
    "- [numpy](www.numpy.org) is the main package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a library to plot graphs in Python.\n",
    "- pytorch\n",
    "- pytorch nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3669,
     "status": "ok",
     "timestamp": 1695233811070,
     "user": {
      "displayName": "Julie Dickerson",
      "userId": "11987863926132313021"
     },
     "user_tz": 300
    },
    "id": "HEBJf8T6kbit"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "\n",
    "np.random.seed(2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJmx89lskbi5"
   },
   "source": [
    "## 2 - Outline of the Assignment\n",
    "\n",
    " Here is an outline of this assignment, you will:\n",
    "\n",
    "1.  Read in the CIFAR-10 training and testing data\n",
    "2.  Review how minibatches are implemented in PyTorch.\n",
    "3.  Create a PyTorch Model of a CNN using the module function in PyTOrch\n",
    "4.  Choose an optimizer and objective function for the model.\n",
    "5.  Train the model.\n",
    "6.  Visual the results at selected layers of the network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dXH3nbBs1ij"
   },
   "source": [
    "### 2.1 Read in and process the data set\n",
    "PyTorch provides data loaders for common data sets used in vision applications, such as MNIST, CIFAR-10 and ImageNet through the [*torchvision*](https://pytorch.org/docs/stable/torchvision/index.html) package. The *torchvision* package consists of popular datasets, model architectures, and common image transformations for computer vision. Other handy tools are the *torch.utils.data.DataLoader* that we will use to load the data set for training and testing and the *torchvision.transforms*, which we will use to compose a two-step process to prepare the data for use with the CNN.\n",
    "\n",
    "First step is to convert Python Image Library (PIL) format\n",
    "to PyTorch tensors.\n",
    "\n",
    "Second step is used to normalize the data by specifying a\n",
    "mean and standard deviation for each of the three channels.\n",
    "This will convert the data from [0,1] to [-1,1]\n",
    "\n",
    "Normalization of data should help speed up conversion and\n",
    "reduce the chance of vanishing gradients with certain\n",
    "activation functions.\n",
    "\n",
    "There are 50000 training examples and 10000 testing examples in the CIFAR-10 dataset at https://www.cs.toronto.edu/~kriz/cifar.html. for more details see: Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009.\n",
    "\n",
    "Code from: [Stefan Fiott](https://www.stefanfiott.com/machine-learning/cifar-10-classifier-using-cnn-in-pytorch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6aaPKv3Utkkq"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Initialize transform function to convert to PyTorch tensors\n",
    "# Normalize: standardizes each channel of the input using a mean of 0.5 and a std\n",
    "# of 0.5\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "print(len(trainset), ' training images')\n",
    "print(trainset.class_to_idx)\n",
    "\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "# Set up a batch data loader for image display\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                         shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=True, num_workers=2)\n",
    "\n",
    "classes = ('airplane', 'automobile', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gdbw-Y0wXNy"
   },
   "source": [
    "### 2.2 Display the images\n",
    "\n",
    "Using the trainloader we will now get a random batch of 4 training images and plot them to see what CIFAR-10 images look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NOf4kxjwV4F"
   },
   "outputs": [],
   "source": [
    "def convert_to_imshow_format(image):\n",
    "    # first convert back to [0,1] range from [-1,1] range\n",
    "    image = image / 2 + 0.5\n",
    "    image = image.numpy()\n",
    "    # convert from CHW to HWC\n",
    "    # from 3x32x32 to 32x32x3\n",
    "    return image.transpose(1,2,0)\n",
    "\n",
    "\n",
    "# can change to testloader to see four test images\n",
    "data_iter = iter(trainloader)\n",
    "images, labels = next(data_iter) \n",
    "\n",
    "fig, axes = plt.subplots(1, len(images), figsize=(12,2.5))\n",
    "for idx, image in enumerate(images):\n",
    "    axes[idx].imshow(convert_to_imshow_format(image))\n",
    "    axes[idx].set_title(classes[labels[idx]])\n",
    "    axes[idx].set_xticks([])\n",
    "    axes[idx].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJawGOMUkbi6"
   },
   "source": [
    "## 3 - Setting up a Simple Convolutional Network\n",
    "\n",
    "The network has the following layout,\n",
    "\n",
    "*Input > Conv (ReLU) > MaxPool > Conv (ReLU) > MaxPool > FC (ReLU) > FC (ReLU) > FC (SoftMax) > 10 outputs*\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "*Conv* is a convolutional layer, *ReLU* is the activation function, *MaxPool* is a pooling layer, *FC* is a fully connected layer and *SoftMax* is the activation function of the output layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDcQ9YlvMlQc"
   },
   "source": [
    "## 3.1 Layer dimensions\n",
    "\n",
    "```\n",
    "Layer Name    | Layer Size\n",
    "______________|______________________________________________________\n",
    "Input         | 32x32x3, i.e., 3 channels each of size 32x32 pixels\n",
    "Conv Layer 1  | 5x5x3 filters, s=1, p=\"same\", 32 slices\n",
    "Max Pool 1    | 2x2 with stride = 2\n",
    "Conv Layer 2  | 3x3x16 filters, s=1, p=\"valid\", 64 slices\n",
    "Max Pool 2    | 2x2 with stride = 2\n",
    "FC 1          |  (1568) x 200 Nodes + bias\n",
    "FC 2          |  200 by 84 Nodes + bias\n",
    "FC 3          |  84 by 10 Nodes + bias\n",
    "Output        | 10 Nodes\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "**PyTorch Functions**\n",
    "\n",
    "*Conv2d(n_input_channels, n_output_channels, f(filter size))*\n",
    "\n",
    "MaxPool2d(size, stride)\n",
    "\n",
    "nn.Linear(input_dim, output_dim)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkQ6ck1K0wqJ"
   },
   "source": [
    "\n",
    "\n",
    "**Fill in the code defining the model below.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BD28TfpVYa_j"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HW3Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HW3Net, self).__init__()\n",
    "        # define each layer here\n",
    "        self.conv1 = None\n",
    "        self.pool =  None\n",
    "        self.conv2 =  None\n",
    "        self.fc1 =  None\n",
    "        self.fc2 =  None\n",
    "        self.fc3 =  None\n",
    "\n",
    "    def forward(self, x):\n",
    "      # Define the forward Path of the model here\n",
    "      # Convolution and pooling layers\n",
    "        # Convolution Layer 1\n",
    "        x = None\n",
    "        # Convolution Layer 2\n",
    "        x = None\n",
    "\n",
    "        # reshape the output for the FC layers\n",
    "        # Hint: replace None with the product of the dimensions of the Conv layer 2 after pooling\n",
    "        x = x.view(-1, None)\n",
    "\n",
    "        # Fully Connected layers\n",
    "        x = None  #FC Layer 1\n",
    "        x = None  #FC Layer 2\n",
    "        x = None  #FC Layer 3\n",
    "        return x\n",
    "\n",
    "# Instantiate your network as net\n",
    "net = HW3Net()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0j76WS9UkbjE"
   },
   "source": [
    "### 3.2 - Defining the Loss Function and Optimization\n",
    "\n",
    "Since we are classifying images into more than two classes we will use cross-entropy as a loss function. To optimize the network we will employ stochastic gradient descent (SGD) with momentum to help get us over local minima and saddle points in the loss function space. Use a learning rate of 0.001 and a momentum value of 0.9.\n",
    "\n",
    "Look at the documentation in torch.nn for the loss function(https://pytorch.org/docs/stable/nn.html#loss-functions) and torch.optim to find the proper optimization function (https://pytorch.org/docs/stable/optim.html).\n",
    "\n",
    "\n",
    "Fill in the code defining the loss function and optimizer below. Try to use at least one type of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uh1vv5H1aaxz"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Select the appropriate criteria from torch.optim package:\n",
    "criterion = None\n",
    "optimizer = None\n",
    "num_epoch = 6 #run for 1 epoch first until you feel good about your model.\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_ppYyUFkbjL"
   },
   "source": [
    "### 3.3 - Training the Network\n",
    "\n",
    "We will now set up to train the network using the trainloader data, by going over all the training data in batches of 16 images, and repeating the whole process num_epoch times. Every 1000 batches (16K images), we report on training progress by printing the current epoch and batch number along with the running loss value.\n",
    "\n",
    "Once training is complete, we will save the model parameters to disk. This will make it possible to load the model parameters from disk the next time we run this notebook and thus not have to train the model again, saving some time. More details on how to save and load model parameters can be found here.\n",
    "\n",
    "\n",
    "\n",
    "Note: Ignore the warning shown below. It is a known bug in PyTorch that was reported this summer.\n",
    "*/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)*\n",
    ".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pwd = os.getcwd()\n",
    "modelpath = \"cifar-10-cnn-model.pt\"\n",
    "full_modelpath = os.path.join(pwd, modelpath)\n",
    "print(full_modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set flags for saving and loading file\n",
    "load_flag = \"FALSE\" #if true, a previously saved model will be loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpP8-5bna7S0"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "# Saving your model is useful if you would like to save your results then go back and train more later.\n",
    "if (load_flag == \"TRUE\"):#os.path.isfile(full_modelpath):\n",
    "    # load trained model parameters from disk\n",
    "    net.load_state_dict(torch.load(full_modelpath))\n",
    "    print('Loaded model parameters from disk, so no need to retrain.')\n",
    "    net.eval()\n",
    "    \n",
    "for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0 # compute loss per epoch\n",
    "    # Loop through iterable containing training data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # forward pass\n",
    "        outputs = None\n",
    "\n",
    "        # compute loss\n",
    "        loss = None\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform backpropagation 2 steps, loss then optimizer\n",
    "        # Loss computation\n",
    "        None\n",
    "        # Optimizer\n",
    "        None\n",
    "\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # print every 1000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 1000))\n",
    "            running_loss = 0.0\n",
    "    print('Finished Epoch %d',(epoch+1))\n",
    "\n",
    "# save the trained model to disk\n",
    "torch.save(net.state_dict(), full_modelpath)\n",
    "print('Saved model parameters to disk.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pj6bAUFiPpNo"
   },
   "source": [
    "## 4. Testing the Model\n",
    "Now that the network is trained, we can evaluate how it performs on the testing data set. Start with four random images from the testing data set and their corresponding labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rEPJ_IPP_jY"
   },
   "source": [
    "**Four Random Images and their class (category) predictions**\n",
    "\n",
    "First, we input four images to the trained network to get class (label/category) predictions.\n",
    "\n",
    "The network outputs a 2D tensor (array) of size 4x10, a row for each image and a column for each category. The values are raw outputs from the linear transformation, $z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]}$\n",
    "\n",
    "The category predicted for each image (row) is thus the column index containing the maximum value in that row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjqaapUKd9wf"
   },
   "outputs": [],
   "source": [
    "data_iter = iter(testloader)\n",
    "images, labels = data_iter.next()\n",
    "\n",
    "fig, axes = plt.subplots(1, len(images), figsize=(12,2.5))\n",
    "for idx, image in enumerate(images):\n",
    "    axes[idx].imshow(convert_to_imshow_format(image))\n",
    "    axes[idx].set_title(classes[labels[idx]])\n",
    "    axes[idx].set_xticks([])\n",
    "    axes[idx].set_yticks([])\n",
    "\n",
    "outputs = net(images)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPuCOoLBepJs"
   },
   "source": [
    " **Probability Score and Class Prediction**\n",
    "\n",
    " To get a probability score, use the *nn.Softmax* function on the raw output as shown below. The predicted category is the one with the maximum probability score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnESdwK6e5LP"
   },
   "outputs": [],
   "source": [
    "sm = nn.Softmax(dim=1)\n",
    "sm_outputs = sm(outputs)\n",
    "print(\"Probability Score:\",sm_outputs)\n",
    "\n",
    "probs, index = torch.max(sm_outputs, dim=1)\n",
    "\n",
    "for p, i in zip(probs, index):\n",
    "    print('{0} - {1:.4f}'.format(classes[i], p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMe6TYswQlQL"
   },
   "source": [
    "**Testing Predictions for all Test Images**\n",
    "\n",
    "We will now loop through all of the test images to see how we did. I got an accuracy of around 65% after 5 epochs, around 70-72% for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RiaHCBOVgQxZ"
   },
   "outputs": [],
   "source": [
    "total_correct = 0\n",
    "total_images = 0\n",
    "confusion_matrix = np.zeros([10,10], int)\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_images += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        for i, l in enumerate(labels):\n",
    "            confusion_matrix[l.item(), predicted[i].item()] += 1\n",
    "\n",
    "model_accuracy = total_correct / total_images * 100\n",
    "print('Model accuracy on {0} test images: {1:.2f}%'.format(total_images, model_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPXJb5F_TdMy"
   },
   "source": [
    "## 5. Visualizing the Results\n",
    "Now that the network is trained, we can visualize the results to investigate the conditions where the model works well and when it makes mistakes using some visualization tools from statistical graphics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gx3d8a-Sgg19"
   },
   "source": [
    "**Confusion Matrix Computation**\n",
    "\n",
    "The model performed much better than random guessing, which would give us an accuracy of 10% since there are ten categories in CIFAR-10. The confusion matrix computes the accuracy of the model per category so we can see if there are particular categories that are being confused with one another. This matrix can be visualized as a heatmap. The squares show the matches between the input images and what the model guessed.\n",
    "\n",
    "What type of image were deer misclassified as most often?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wT-yI_Brg3-Y"
   },
   "outputs": [],
   "source": [
    "print('{0:10s} - {1}'.format('Category','Accuracy'))\n",
    "for i, r in enumerate(confusion_matrix):\n",
    "    print('{0:10s} - {1:.1f}'.format(classes[i], r[i]/np.sum(r)*100))\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "ax.matshow(confusion_matrix, aspect='auto', vmin=0, vmax=1000, cmap=plt.get_cmap('Blues'))\n",
    "plt.ylabel('Actual Category')\n",
    "plt.yticks(range(10), classes)\n",
    "plt.xlabel('Predicted Category')\n",
    "plt.xticks(range(10), classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CB4DDT4FhQg8"
   },
   "source": [
    "**Confusion Matrix**\n",
    "\n",
    "From the above heatmap visualisation, we can see where the best accuracy was achieved, the darkest shades present on the main diagonal.\n",
    "\n",
    "To understand precisely which categories were most commonly confused, we can print the percentage and absolute values of the confusion matrix below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tkXI5N1jhs-V"
   },
   "outputs": [],
   "source": [
    "print('actual/pred'.ljust(16), end='')\n",
    "for i,c in enumerate(classes):\n",
    "    print(c.ljust(10), end='')\n",
    "print()\n",
    "for i,r in enumerate(confusion_matrix):\n",
    "    print(classes[i].ljust(16), end='')\n",
    "    for idx, p in enumerate(r):\n",
    "        print(str(p).ljust(10), end='')\n",
    "    print()\n",
    "\n",
    "    r = r/np.sum(r)\n",
    "    print(''.ljust(16), end='')\n",
    "    for idx, p in enumerate(r):\n",
    "        print(str(p).ljust(10), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZA23BvfzU9Cs"
   },
   "source": [
    "# 6. Analysis Questions\n",
    "\n",
    "Your homework score will be based on \n",
    "1. the coding of the model (20 points) in sections 3.1, 3.2 and 3.3. \n",
    "2. the answers of the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Convolution Nets Questions (20 points)\n",
    "**Question 1** \n",
    "What categories of images were identified most accurately? Which categories were confused with each other more often? Use your results from Part 5 to inform your answers. Speculate on why some categories of image were more difficult to work with.\n",
    "\n",
    "**Question 2** Compare your results with those of HW 3 in terms of accuracy across classes, speed of training, and number of parameters. \n",
    "\n",
    "**Question 3** Describe two methods for data augmentation and how they might improve system performance. \n",
    "\n",
    "**Extra Credit** (Up to 5 additional points)\n",
    "If you have time, try to implement increasing the size of the training data set by using the PyTorch data augmentation code See <a href=\"https://pytorch.org/vision/0.15/transforms.html\" target=\"_blank\" rel=\"noopener\">Transforming and Augmenting Images</a>. There are some good tutorials on the PyTorch site for you to review. You must cite any tutorials/examples that you used and clearly state which type of augmentation you tried."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Concept Questions (10 points)\n",
    "Answer each question and give an explanation for why it is true.\n",
    "\n",
    "1. Which of the following are true of pooling layers in CNNs?\n",
    "a. They reduce the size of the input to the next layer.\n",
    "b. They increase the number of parameters.\n",
    "c. They reduce the number of connections to the next layer.\n",
    "d. They reduce the number of parameters\n",
    "\n",
    "2. Which of the following are true of convolutional neural networks for image analysis?\n",
    "\n",
    "a. Filters in earlier layers tend to include edge detectors.\n",
    "b. Pooling layers reduce the spatial resolution of the image.\n",
    "c. They have more parameters than fully-connected networks with the\n",
    "same number of layers and the same number of neurons in each layer.\n",
    "d. A CNN can be trained for unsupervised learning tasks, whereas an ordinary neural net cannot.\n",
    "\n",
    "3. What is the valid convolution of the filter F with image X?\n",
    "$$X = \\begin{bmatrix}1 & 1 & 4 & 4\\\\\n",
    "1 & 1 & 4 & 4\\\\\n",
    "4 & 4 & 1 & 1\\\\\n",
    "4 & 4 & 1 & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$K = \\begin{bmatrix}1 & 1 & 1\\\\\n",
    "1 & -6 & 1\\\\\n",
    "1 & 1 & 1\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "4.What is the result of performing **average pooling** on the image X? Use a 2 x 2 kernel, with stride of 2, and padding with values of (1, 1) pixel on the right side and bottom of the image.\n",
    "\n",
    "$$X = \\begin{bmatrix}1 & 17 & 43 & 4 & 5\\\\\n",
    "2 & 2 & 6 & 8 & 7\\\\\n",
    "12 & 9 & 4 & 45 & 5\\\\\n",
    "3 & 4 & 78 & 9 & 62\\\\\n",
    "12 & 11 & 14 & 42 & 15\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1IzgpktsaK0QN672uqn5wDNE2VJ9VD48K",
     "timestamp": 1633483427135
    },
    {
     "file_id": "11vzlWQ4Cz-X3FOTTHdy-K-rk-oOpTADw",
     "timestamp": 1601044270147
    },
    {
     "file_id": "1_95JnBIPRVmbwZKCFt7y9OeVPIYJxv08",
     "timestamp": 1600058124558
    },
    {
     "file_id": "1ro17XxjOIFChte0wBgGKQ5IyWZZcZsMO",
     "timestamp": 1599857656680
    },
    {
     "file_id": "1j42Grde7GbmW6xu4KvThnwodfydDKTEy",
     "timestamp": 1599499755884
    }
   ]
  },
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
